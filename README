

This works only on ubuntu lucid boxes.

1. Clone
2. 
   a. Download an oracle-jdk binary (eg http://www.oracle.com/technetwork/java/javase/downloads/jdk-6u31-download-1501634.html) to the jajc directory 
   b. Download ActiveMQ from apache, place the tar.gz file in the jajc directory as activemq.tgz, it must match this name for now.

Make sure you download the correct version: certain images on ec2 are 64bit only, some are 32bit, others support both.



3. jajc/setup.yml is the file that contains the configuration for the cluster
   a. If you're going to use openjdk set installopenjdk to "true" in the java section of the userConfig. If not, make sure you specify the filename of the file you just downloaded eg: bin_filename: "jdk-6u31-linux-i586.bin". In the case of openjdk you must set the install_path to the location of JAVA_HOME after installing the openjdk package (so you must know this in advance!)
   b. Set the provider
      * byon 
      	- fully supported and mcollective commands should work
      	- In the instances section you must provide the hostnames of the nodes, see the example, the number value will be ignored.
	- Properties that are the same for all the instances can be specified in the byon section of providerSpecificInfo. You can override them in options section of an instance.

      * aws-ec2
        - the java binary (if you don't use openjdk) and activemq.tgz will be uploaded to the puppetmaster, this can take very long.
	- mcollective is unsupported since we use internal hostnames on ec2
	- You don't need to provide hostnames in the instances section, the number value can be used to create multiple machines with the specified roles.
	- In the awsec2 section of providerSpecificInfo the following values must be set : securtity_group, keypair_name, accesskeyid, secretkey, private_key, this last one is the content of the private keyfile corresponding with the keypair_name, see the setupec2.yml example file.


   c. The roles are specified as puppet classes, at the moment these are available:
      puppetmaster, cdh3::hadoop::namenode, cdh3::hadoop::datanode, cdh3::zookeeper, cdh3::hbase::master, cdh3::hbase::regionserver, cdh3::hadoop::jobtracker, cdh3::hadoop::tasktracker, lily::server, lily::solr. Make sure you specify at least one puppetmaster, a namenode, a jobtracker and a zookeeper.

   d. Specify your lily repository credentials in the lily-section of the userConfig, if you don't use lily, you can give bogus info here, they are still required though.
   e. You can add as many options as you want for the cdh3 services in their respective sections in userConfig, they will be set accordingly when the cluster starts.
   
4. No longer necessary

5. In the jajc directory : execute "mvn exec:java", there're two possible options: location of the setup.yml file and location of the puppet modules directory: eg 'mvn exec:java -Dexec.args="--yml-file=/home/username/mysetup.yml" -Dexec.args="--moduledir=/tmp/test/modules'. The modules directory must be named "modules"!!

After a while the build will be completed and puppet should run on the cluster.

To see what puppet is doing and if everything is going ok, you can log in to a node of the cluster and tail -f /var/log/syslog, this is where you can see the progress of the puppet agent.


After the puppet run is complete (there shouldn't have been any errors) it might be possible that all the services have started correctly since most services keep retrying to start. This is no guarantee and it's usually better to execute the mcollective stopall.sh script in the scripts directory, this will stop all the services. Using startall.sh you can start the services. Make sure the read the README in the scripts directory, it contains information on how to setup the mcollective client, used to sent messages to the cluster, on your computer.




CREDITS:
apt module from https://github.com/camptocamp/puppet-apt
